import io
import re
import pymorphy3
import unicodedata
import numpy as np
import pandas as pd
from PIL import Image
import streamlit as st
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity


class MemeSearchPreprocessor:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–µ–º–æ–≤.
    –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è –º–æ–¥–µ–ª–∏ paraphrase-multilingual-MiniLM-L12-v2.
    """
    
    def __init__(self, use_lemmatization: bool = True, fix_typos: bool = True):
        """
        Args:
            use_lemmatization: –ø—Ä–∏–º–µ–Ω—è—Ç—å –ª–∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é
            fix_typos: –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å –ª–∏ —á–∞—Å—Ç—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏
        """
        self.use_lemmatization = use_lemmatization
        self.fix_typos = fix_typos
        
        if self.use_lemmatization:
            self.morph = pymorphy3.MorphAnalyzer()
        
        self.typo_dict = {
            '—à—Ç–æ–∂': '—á—Ç–æ –∂', '—á—ë': '—á—Ç–æ', '—â–∞—Å': '—Å–µ–π—á–∞—Å', '—â–∞': '—Å–µ–π—á–∞—Å',
            '—Å–ø—Å': '—Å–ø–∞—Å–∏–±–æ', '–ø–ª–∏–∑': '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞', '–æ–∫': '–æ–∫–µ–π',
            '–ø–∞—Å–∏–±': '—Å–ø–∞—Å–∏–±–æ', '–ø—Ä–∏–≤': '–ø—Ä–∏–≤–µ—Ç', '–ø–∂': '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞',
            '–ø–∂–ª': '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞', '–ø–∂–ª—Å—Ç': '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞',
            '—Ä—É–¥–¥—â': '–ø—Ä–∏–≤–µ—Ç', '—Ä—É–¥–∑': '–ø—Ä–∏–≤–µ—Ç', '–ø—Ñ': '–∞–ø',
            '–∑—â': '—è–∏', '—â—Ç': '—à—Ç',
            '–∏–º—Ö–æ': '–ø–æ –º–æ–µ–º—É –º–Ω–µ–Ω–∏—é', '–ª–æ–ª': '—Å–º–µ—à–Ω–æ', '–∫–µ–∫': '—Å–º–µ—à–Ω–æ',
            '—Ä–æ—Ñ–ª': '–æ—á–µ–Ω—å —Å–º–µ—à–Ω–æ', '–æ–º–≥': '–æ –±–æ–∂–µ', '–Ω–Ω': '–Ω–æ—Ä–º–∞–ª—å–Ω–æ',
            '—Ö–∑': '–Ω–µ –∑–Ω–∞—é', '–∏–∑–∏': '–ª–µ–≥–∫–æ', '–≥–≥': '—Ö–æ—Ä–æ—à–∞—è –∏–≥—Ä–∞',
            '–≤–ø–Ω': 'vpn', '–∏–¥': '–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä',
            'cry': '–ø–ª–∞–∫–∞—Ç—å', 'lol': '—Å–º–µ—è—Ç—å—Å—è', 'omg': '–æ –±–æ–∂–µ',
            'wtf': '—á—Ç–æ –∑–∞ —á–µ—Ä—Ç', 'brb': '—Å–∫–æ—Ä–æ –≤–µ—Ä–Ω—É—Å—å', 'idk': '–Ω–µ –∑–Ω–∞—é',
            '—Å–∏–º–ø–æ—Ç–Ω—ã–π': '—Å–∏–º–ø–∞—Ç–∏—á–Ω—ã–π', '–∑–¥–µ–ª–∞—Ç—å': '—Å–¥–µ–ª–∞—Ç—å',
            '–≤–æ–æ–±—â–µ–º': '–≤ –æ–±—â–µ–º', '–∏—Ö–Ω–∏–π': '–∏—Ö', '–ª–æ–∂–∏—Ç—å': '–∫–ª–∞—Å—Ç—å',
            '–µ–∑–¥–∏—Ç—å': '–µ—Ö–∞—Ç—å', '–∫–æ–æ—Ä–¥–∏–Ω–∞–ª—å–Ω–æ': '–∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ',
        }
        
        self.do_not_lemmatize = {
            '–¥–æ–≥e', '–∫–æ—Ç—ç', '–ø—ë—Å–µ–ª—å', '–∫–æ—Ç–µ–π–∫–∞', '–ø—Å–∏–Ω–∞',
            '–ø—Ä–µ–≤–µ–¥', '–º–µ–¥–≤–µ–¥', '–∂–æ–∂—ã–∫', '–∫—Ä–æ—Å–∞–≤—á–µ–≥',
            '–∞–Ω–∏–º–µ', '–º–µ–º', '–≥–∏—Ñ', '—Å—Ç—Ä–∏–º', '—Å—Ç—Ä–∏–º–µ—Ä',
            '—é—Ç—É–±', '—Ç–∏–∫—Ç–æ–∫', '–∏–Ω—Å—Ç–∞–≥—Ä–∞–º',
            '–ø—É—Ç–∏–Ω', '—Ç—Ä–∞–º–ø', '–±–∞–π–¥–µ–Ω', '–º–∞—Å–∫', '–æ–±–Ω–∏–º–∞',
            '–ø–µ–ø–µ', '–¥–æ–≥–µ', '–¥–æ–∂', '–∂–æ–∂',
        }
    
    def normalize_text(self, text: str) -> str:
        """–ë–∞–∑–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        if not text or not isinstance(text, str):
            return ""
        
        text = unicodedata.normalize('NFKC', text)
        text = text.lower()
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def remove_special_chars(self, text: str, keep_hashtags: bool = True) -> str:
        """
        –£–¥–∞–ª—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–º—ã—Å–ª.
        
        Args:
            keep_hashtags: —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ö—ç—à—Ç–µ–≥–∏
        """
        if keep_hashtags:
            hashtags = re.findall(r'#\w+', text)
            text = re.sub(r'#\w+', ' HASHTAG_PLACEHOLDER ', text)
        
        text = re.sub(r'https?://\S+|www\.\S+', ' URL_PLACEHOLDER ', text)
        text = re.sub(r'\S+@\S+', ' EMAIL_PLACEHOLDER ', text)
        
        smileys = re.findall(r'[:;=]-?[\)\(/\\\]\[DPp]', text)
        text = re.sub(r'[:;=]-?[\)\(/\\\]\[DPp]', ' SMILEY_PLACEHOLDER ', text)
        
        text = re.sub(r'[^\w\s\-\'.,!?]', ' ', text)
        
        if keep_hashtags and hashtags:
            for ht in hashtags:
                text = text.replace('HASHTAG_PLACEHOLDER', ht.lower(), 1)
        
        if smileys:
            for sm in smileys:
                text = text.replace('SMILEY_PLACEHOLDER', sm, 1)
        
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def fix_common_typos(self, text: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç —á–∞—Å—Ç—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏"""
        if not self.fix_typos:
            return text
        
        words = text.split()
        corrected_words = []
        
        for word in words:
            if word in self.typo_dict:
                corrected_words.append(self.typo_dict[word])
            else:
                corrected_words.append(word)
        
        return ' '.join(corrected_words)
    
    def smart_lemmatization(self, text: str) -> str:
        """
        –£–º–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å –∏—Å–∫–ª—é—á–µ–Ω–∏—è–º–∏ –¥–ª—è –º–µ–º-–∫—É–ª—å—Ç—É—Ä—ã.
        """
        if not self.use_lemmatization or not hasattr(self, 'morph'):
            return text
        
        words = text.split()
        lemmatized_words = []
        
        for word in words:
            if any(ph in word for ph in ['PLACEHOLDER', 'URL', 'EMAIL', 'HASHTAG', 'SMILEY']):
                lemmatized_words.append(word)
                continue
            
            if word.lower() in self.do_not_lemmatize:
                lemmatized_words.append(word)
                continue
            
            if re.match(r'^[a-zA-Z]+$', word):
                lemmatized_words.append(word.lower())
                continue
            
            try:
                parsed = self.morph.parse(word)[0]
                lemma = parsed.normal_form
                
                if word[0].isupper() and len(word) > 1:
                    lemma = lemma.capitalize()
                
                lemmatized_words.append(lemma)
            except:
                lemmatized_words.append(word)
        
        return ' '.join(lemmatized_words)
    
    def preprocess(self, text: str, for_search: bool = True) -> str:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏.
        
        Args:
            text: –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
            for_search: True –µ—Å–ª–∏ —ç—Ç–æ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å, False –µ—Å–ª–∏ —ç—Ç–æ —Ç–µ–∫—Å—Ç –º–µ–º–∞
        """
        if not text:
            return ""
        
        text = self.normalize_text(text)
        
        if self.fix_typos:
            text = self.fix_common_typos(text)
        
        text = self.remove_special_chars(text, keep_hashtags=not for_search)
        
        if self.use_lemmatization:
            text = self.smart_lemmatization(text)
        
        text = re.sub(r'\s+', ' ', text).strip()
        
        words = text.split()
        if len(words) > 50:
            text = ' '.join(words[:50])
        
        return text
    
    def preprocess_batch(self, texts: list[str], for_search: bool = True) -> list[str]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        return [self.preprocess(text, for_search) for text in texts]


class MemeSearchEngine:
    def __init__(self, data_path: str, model_name: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞.
        
        Args:
            data_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ (.parquet)
            model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        self.df = pd.read_parquet(data_path)
        self.model = SentenceTransformer(model_name)
        self.meme_embeddings = np.array(self.df['embedding'].tolist())
        self.preprocessor = MemeSearchPreprocessor()
    
    def search(self, query: str, top_k: int = 5, min_similarity: float = 0.0) -> pd.DataFrame:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏.
        
        Args:
            query: —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_similarity: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.0-1.0)
                
        Returns:
            DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∏–º–∏ –æ–±–æ–∏–º —É—Å–ª–æ–≤–∏—è–º
        """
        query_processed = self.preprocessor.preprocess(query, for_search=True)
        query_embedding = self.model.encode([query_processed], convert_to_numpy=True)
        similarities = cosine_similarity(query_embedding, self.meme_embeddings)[0]

        if min_similarity > 0:
            mask = similarities >= min_similarity
            eligible_indices = np.where(mask)[0]
        else:
            eligible_indices = np.arange(len(similarities))
        
        if len(eligible_indices) == 0:
            return pd.DataFrame()

        sorted_indices = eligible_indices[np.argsort(similarities[eligible_indices])[::-1]]
        k = min(int(top_k), len(sorted_indices))
        top_indices = sorted_indices[:k]

        results = []
        for idx in top_indices:
            row = self.df.iloc[idx].copy()
            row['score'] = float(similarities[idx])
            results.append(row)
        
        return pd.DataFrame(results)
    
    def get_image_bytes(self, idx: int) -> bytes:
        """–ü–æ–ª—É—á–∏—Ç—å –±–∞–π—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ –∏–Ω–¥–µ–∫—Å—É –≤ DataFrame."""
        return self.df.iloc[idx]['local_path']


st.set_page_config(
    page_title="üîç –ü–æ–∏—Å–∫ –º–µ–º–æ–≤ –ø–æ —Å–º—ã—Å–ª—É",
    page_icon="ü§ñ",
    layout="wide"
)

@st.cache_resource
def load_engine():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    return MemeSearchEngine('data/memes_post.parquet')

def load_css():
    st.markdown("""
    <style>
    .main-header {
        font-size: 2rem;
        color: #1E3A8A;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    .subheader {
        font-size: 1rem;
        color: #6B7280;
        text-align: center;
        margin-bottom: 1rem;
    }
    .meme-container {
        margin-bottom: 1.5rem;
    }
    .score-badge {
        display: inline-block;
        padding: 0.2rem 0.6rem;
        border-radius: 12px;
        font-size: 0.8rem;
        font-weight: bold;
        margin-top: 0.3rem;
    }
    .high-score { background-color: #D1FAE5; color: #065F46; }
    .medium-score { background-color: #FEF3C7; color: #92400E; }
    .low-score { background-color: #FEE2E2; color: #991B1B; }
    </style>
    """, unsafe_allow_html=True)

def get_score_badge_class(score):
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ CSS-–∫–ª–∞—Å—Å–∞ –¥–ª—è –±–µ–π–¥–∂–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ—Ü–µ–Ω–∫–∏"""
    if score >= 0.7:
        return "high-score"
    elif score >= 0.4:
        return "medium-score"
    else:
        return "low-score"

def main():
    load_css()
    
    st.markdown('<h1 class="main-header">üîç –ü–æ–∏—Å–∫ –º–µ–º–æ–≤ –ø–æ —Å–º—ã—Å–ª—É</h1>', unsafe_allow_html=True)
    st.markdown('<p class="subheader">–û–ø–∏—à–∏—Ç–µ —Å–∏—Ç—É–∞—Ü–∏—é –Ω–∞ —Ä—É—Å—Å–∫–æ–º ‚Äî –Ω–∞–π–¥–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π –º–µ–º</p>', unsafe_allow_html=True)
    
    with st.sidebar:
        st.header("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞")
        
        search_mode = st.radio(
            "–†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞:",
            ["–ü–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É", "–ü–æ —Å—Ö–æ–∂–µ—Å—Ç–∏", "–ì–∏–±—Ä–∏–¥–Ω—ã–π"],
            index=0
        )
        
        if search_mode == "–ü–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É":
            top_k = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", 1, 15, 6)
            min_similarity = 0.0
        elif search_mode == "–ü–æ —Å—Ö–æ–∂–µ—Å—Ç–∏":
            min_similarity = st.slider("–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å", 0.0, 1.0, 0.3, 0.05)
            top_k = 1000
        else:
            top_k = st.slider("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ", 1, 15, 6)
            min_similarity = st.slider("–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å", 0.0, 1.0, 0.3, 0.05)
        
        st.divider()
        st.subheader("–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
        layout_cols = st.selectbox("–ö–æ–ª–æ–Ω–æ–∫ –≤ —Å—Ç—Ä–æ–∫–µ", [2, 3, 4], index=1)
        
        st.divider()
        st.subheader("‚ÑπÔ∏è –û —Å–∏—Å—Ç–µ–º–µ")
        st.markdown("""
        **–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:**
        - –ú–æ–¥–µ–ª—å: `paraphrase-multilingual-MiniLM-L12-v2`
        - –ü–æ–∏—Å–∫: –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
        - –ë–∞–∑–∞: 2.3k —à–∞–±–ª–æ–Ω–æ–≤ –º–µ–º–æ–≤
        
        **–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**
        1. –í–∞—à –∑–∞–ø—Ä–æ—Å –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –≤ –≤–µ–∫—Ç–æ—Ä
        2. –ò—â—É—Ç—Å—è –±–ª–∏–∑–∫–∏–µ –≤–µ–∫—Ç–æ—Ä—ã –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –º–µ–º–æ–≤
        3. –í–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ
        
        **–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:**
        - –ö—Ä–æ—Å—Å-—è–∑—ã–∫–æ–≤–æ–π –ø–æ–∏—Å–∫ (—Ä—É—Å—Å–∫–∏–π ‚Üí –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
        - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å–º—ã—Å–ª–∞
        - 3 —Ä–µ–∂–∏–º–∞ –ø–æ–∏—Å–∫–∞
        """)
    
    engine = load_engine()
    
    col1, col2 = st.columns([3, 1])
    with col1:
        query = st.text_input(
            "–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å:",
            placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: '–∫–æ—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–ª–∞–Ω–∏—Ä—É–µ—Ç –º–µ—Å—Ç—å'",
            key='query_input'
        )
    with col2:
        st.write("")
        st.write("")
        search_clicked = st.button("–ò—Å–∫–∞—Ç—å", use_container_width=True, type="primary")
    
    if search_clicked and query:
        with st.spinner("–ò—â–µ–º..."):
            results = engine.search(query, top_k=top_k, min_similarity=min_similarity)
        
        st.session_state.results = results
        st.session_state.last_query = query
        st.session_state.layout_cols = layout_cols
    
    if hasattr(st.session_state, 'results') and not st.session_state.results.empty:
        results = st.session_state.results
        query = st.session_state.last_query
        layout_cols = st.session_state.get('layout_cols', 3)
        
        st.write("---")
        cols = st.columns(4)
        with cols[0]:
            st.metric("–ù–∞–π–¥–µ–Ω–æ", len(results))
        with cols[1]:
            avg_score = results['score'].mean()
            st.metric("–°—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å", f"{avg_score:.3f}")
        with cols[2]:
            st.metric("–ú–∞–∫—Å–∏–º—É–º", f"{results['score'].max():.3f}")
        with cols[3]:
            st.metric("–ú–∏–Ω–∏–º—É–º", f"{results['score'].min():.3f}")
        
        st.write(f"### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è: '{query}'")
        
        results_list = list(results.iterrows())
        
        for i in range(0, len(results_list), layout_cols):
            cols = st.columns(layout_cols)
            
            for col_idx in range(layout_cols):
                if i + col_idx < len(results_list):
                    idx, row = results_list[i + col_idx]
                    
                    with cols[col_idx]:
                        st.markdown('<div class="meme-container">', unsafe_allow_html=True)
                        
                        try:
                            image = Image.open(io.BytesIO(row['local_path']))
                            st.image(image, use_container_width=True)
                            
                            st.markdown(f"**{row['name']}**")
                            
                            badge_class = get_score_badge_class(row['score'])
                            st.markdown(
                                f'<div class="score-badge {badge_class}">'
                                f'–°—Ö–æ–∂–µ—Å—Ç—å: {row["score"]:.3f}'
                                f'</div>',
                                unsafe_allow_html=True
                            )
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)[:50]}")
                        
                        st.markdown('</div>', unsafe_allow_html=True)
        
        if st.button("üîÑ –ù–æ–≤—ã–π –ø–æ–∏—Å–∫", use_container_width=True):
            st.session_state.pop('results', None)
            st.session_state.pop('last_query', None)
            st.rerun()
    
    elif hasattr(st.session_state, 'results') and st.session_state.results.empty:
        st.warning("–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        
        with st.expander("–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤"):
            st.write("- **–≥—Ä—É—Å—Ç–Ω—ã–π –∫–æ—Ç** ‚Üí sad cat")
            st.write("- **—Ä–∞–¥–æ—Å—Ç—å –ø–æ–±–µ–¥—ã** ‚Üí success kid")
            st.write("- **—É–¥–∏–≤–ª–µ–Ω–∏–µ** ‚Üí surprised pikachu")
            st.write("- **—Ä–∞–±–æ—Ç–∞ –∑–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º** ‚Üí programmer")
            st.write("- **—É—Å—Ç–∞–ª–æ—Å—Ç—å** ‚Üí tired")
            st.write("- **—Å–º–µ—à–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è** ‚Üí funny situation")
    
    st.write("---")
    st.caption(f"¬© 2025 NIR Meme Search ‚Ä¢ OmSTU")

if __name__ == "__main__":
    main()
